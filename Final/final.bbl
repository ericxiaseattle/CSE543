\begin{thebibliography}{16}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Gower et~al.(2020)Gower, Sebbouh, and
  Loizou]{https://doi.org/10.48550/arxiv.2006.10311}
Robert~M. Gower, Othmane Sebbouh, and Nicolas Loizou.
\newblock Sgd for structured nonconvex functions: Learning rates, minibatching
  and interpolation, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.10311}.

\bibitem[Fabian et~al.(2010)Fabian, Henrion, Kruger, and
  Outrata]{Fabian2010ErrorBN}
Marian~J. Fabian, Ren{\'e} Henrion, Alexander~Y. Kruger, and Jir{\'i}~V.
  Outrata.
\newblock Error bounds: Necessary and sufficient conditions.
\newblock \emph{Set-Valued and Variational Analysis}, 18:\penalty0 121--149,
  2010.

\bibitem[Hinder et~al.(2019)Hinder, Sidford, and
  Sohoni]{https://doi.org/10.48550/arxiv.1906.11985}
Oliver Hinder, Aaron Sidford, and Nimit~S. Sohoni.
\newblock Near-optimal methods for minimizing star-convex functions and beyond,
  2019.
\newblock URL \url{https://arxiv.org/abs/1906.11985}.

\bibitem[Hardt et~al.(2018)Hardt, Ma, and Recht]{JMLR:v19:16-465}
Moritz Hardt, Tengyu Ma, and Benjamin Recht.
\newblock Gradient descent learns linear dynamical systems.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (29):\penalty0 1--44, 2018.
\newblock URL \url{http://jmlr.org/papers/v19/16-465.html}.

\bibitem[Guminov et~al.(2017)Guminov, Gasnikov, and
  Kuruzov]{https://doi.org/10.48550/arxiv.1710.00797}
Sergey Guminov, Alexander Gasnikov, and Ilya Kuruzov.
\newblock Accelerated methods for $α$-weakly-quasi-convex problems, 2017.
\newblock URL \url{https://arxiv.org/abs/1710.00797}.

\bibitem[Anitescu(2000)]{doi:10.1137/S1052623499359178}
Mihai Anitescu.
\newblock Degenerate nonlinear programming with a quadratic growth condition.
\newblock \emph{SIAM Journal on Optimization}, 10\penalty0 (4):\penalty0
  1116--1135, 2000.
\newblock \doi{10.1137/S1052623499359178}.
\newblock URL \url{https://doi.org/10.1137/S1052623499359178}.

\bibitem[Polyak(1963)]{POLYAK1963864}
B.T. Polyak.
\newblock Gradient methods for the minimisation of functionals.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  3\penalty0 (4):\penalty0 864--878, 1963.
\newblock ISSN 0041-5553.
\newblock \doi{https://doi.org/10.1016/0041-5553(63)90382-3}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0041555363903823}.

\bibitem[Gower et~al.(2021)Gower, Richt{\'a}rik, and
  Bach]{Gower2021StochasticQM}
R.~Michael Gower, Peter Richt{\'a}rik, and Francis~R. Bach.
\newblock Stochastic quasi-gradient methods: variance reduction via jacobian
  sketching.
\newblock \emph{Mathematical Programming}, 188:\penalty0 135 -- 192, 2021.

\bibitem[Liu et~al.(2020)Liu, Gao, and Yin]{NEURIPS2020_d3f5d4de}
Yanli Liu, Yuan Gao, and Wotao Yin.
\newblock An improved analysis of stochastic gradient descent with momentum.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 18261--18271. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/d3f5d4de09ea19461dab00590df91e4f-Paper.pdf}.

\bibitem[Yu et~al.(2019)Yu, Jin, and
  Yang]{https://doi.org/10.48550/arxiv.1905.03817}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization, 2019.
\newblock URL \url{https://arxiv.org/abs/1905.03817}.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and
  Yang]{https://doi.org/10.48550/arxiv.1808.10396}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock 2018.
\newblock \doi{10.48550/ARXIV.1808.10396}.
\newblock URL \url{https://arxiv.org/abs/1808.10396}.

\bibitem[Bottou et~al.(2016)Bottou, Curtis, and
  Nocedal]{https://doi.org/10.48550/arxiv.1606.04838}
Léon Bottou, Frank~E. Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning, 2016.
\newblock URL \url{https://arxiv.org/abs/1606.04838}.

\bibitem[Shi et~al.(2021)Shi, Li, Hong, and Sun]{shi2021rmsprop}
Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun.
\newblock {RMS}prop converges with proper hyper-parameter.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=3UDSdyIcBDA}.

\bibitem[Défossez et~al.(2020)Défossez, Bottou, Bach, and
  Usunier]{https://doi.org/10.48550/arxiv.2003.02395}
Alexandre Défossez, Léon Bottou, Francis Bach, and Nicolas Usunier.
\newblock A simple convergence proof of adam and adagrad, 2020.
\newblock URL \url{https://arxiv.org/abs/2003.02395}.

\bibitem[De et~al.(2018)De, Mukherjee, and
  Ullah]{https://doi.org/10.48550/arxiv.1807.06766}
Soham De, Anirbit Mukherjee, and Enayat Ullah.
\newblock Convergence guarantees for rmsprop and adam in non-convex
  optimization and an empirical comparison to nesterov acceleration, 2018.
\newblock URL \url{https://arxiv.org/abs/1807.06766}.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and
  Kumar]{https://doi.org/10.48550/arxiv.1904.09237}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond, 2019.
\newblock URL \url{https://arxiv.org/abs/1904.09237}.

\end{thebibliography}
