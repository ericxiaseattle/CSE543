\begin{thebibliography}{31}
\providecommand{\natexlab}[1]{#1}
\providecommand{\url}[1]{\texttt{#1}}
\expandafter\ifx\csname urlstyle\endcsname\relax
  \providecommand{\doi}[1]{doi: #1}\else
  \providecommand{\doi}{doi: \begingroup \urlstyle{rm}\Url}\fi

\bibitem[Gower et~al.(2020)Gower, Sebbouh, and
  Loizou]{https://doi.org/10.48550/arxiv.2006.10311}
Robert~M. Gower, Othmane Sebbouh, and Nicolas Loizou.
\newblock Sgd for structured nonconvex functions: Learning rates, minibatching
  and interpolation, 2020.
\newblock URL \url{https://arxiv.org/abs/2006.10311}.

\bibitem[Fabian et~al.(2010)Fabian, Henrion, Kruger, and
  Outrata]{Fabian2010ErrorBN}
Marian~J. Fabian, Ren{\'e} Henrion, Alexander~Y. Kruger, and Jir{\'i}~V.
  Outrata.
\newblock Error bounds: Necessary and sufficient conditions.
\newblock \emph{Set-Valued and Variational Analysis}, 18:\penalty0 121--149,
  2010.

\bibitem[Hinder et~al.(2019)Hinder, Sidford, and
  Sohoni]{https://doi.org/10.48550/arxiv.1906.11985}
Oliver Hinder, Aaron Sidford, and Nimit~S. Sohoni.
\newblock Near-optimal methods for minimizing star-convex functions and beyond,
  2019.
\newblock URL \url{https://arxiv.org/abs/1906.11985}.

\bibitem[Hardt et~al.(2018)Hardt, Ma, and Recht]{JMLR:v19:16-465}
Moritz Hardt, Tengyu Ma, and Benjamin Recht.
\newblock Gradient descent learns linear dynamical systems.
\newblock \emph{Journal of Machine Learning Research}, 19\penalty0
  (29):\penalty0 1--44, 2018.
\newblock URL \url{http://jmlr.org/papers/v19/16-465.html}.

\bibitem[Guminov et~al.(2017)Guminov, Gasnikov, and
  Kuruzov]{https://doi.org/10.48550/arxiv.1710.00797}
Sergey Guminov, Alexander Gasnikov, and Ilya Kuruzov.
\newblock Accelerated methods for $\alpha$-weakly-quasi-convex problems, 2017.
\newblock URL \url{https://arxiv.org/abs/1710.00797}.

\bibitem[Anitescu(2000)]{doi:10.1137/S1052623499359178}
Mihai Anitescu.
\newblock Degenerate nonlinear programming with a quadratic growth condition.
\newblock \emph{SIAM Journal on Optimization}, 10\penalty0 (4):\penalty0
  1116--1135, 2000.
\newblock \doi{10.1137/S1052623499359178}.
\newblock URL \url{https://doi.org/10.1137/S1052623499359178}.

\bibitem[Polyak(1963)]{POLYAK1963864}
B.T. Polyak.
\newblock Gradient methods for the minimisation of functionals.
\newblock \emph{USSR Computational Mathematics and Mathematical Physics},
  3\penalty0 (4):\penalty0 864--878, 1963.
\newblock ISSN 0041-5553.
\newblock \doi{https://doi.org/10.1016/0041-5553(63)90382-3}.
\newblock URL
  \url{https://www.sciencedirect.com/science/article/pii/0041555363903823}.

\bibitem[Gower et~al.(2021)Gower, Richt{\'a}rik, and
  Bach]{Gower2021StochasticQM}
R.~Michael Gower, Peter Richt{\'a}rik, and Francis~R. Bach.
\newblock Stochastic quasi-gradient methods: variance reduction via jacobian
  sketching.
\newblock \emph{Mathematical Programming}, 188:\penalty0 135 -- 192, 2021.

\bibitem[Niu et~al.(2011)Niu, Recht, Re, and
  Wright]{https://doi.org/10.48550/arxiv.1106.5730}
Feng Niu, Benjamin Recht, Christopher Re, and Stephen~J. Wright.
\newblock Hogwild!: A lock-free approach to parallelizing stochastic gradient
  descent, 2011.
\newblock URL \url{https://arxiv.org/abs/1106.5730}.

\bibitem[Hazan and Kale(2014)]{JMLR:v15:hazan14a}
Elad Hazan and Satyen Kale.
\newblock Beyond the regret minimization barrier: Optimal algorithms for
  stochastic strongly-convex optimization.
\newblock \emph{Journal of Machine Learning Research}, 15\penalty0
  (71):\penalty0 2489--2512, 2014.
\newblock URL \url{http://jmlr.org/papers/v15/hazan14a.html}.

\bibitem[Rakhlin et~al.(2012)Rakhlin, Shamir, and
  Sridharan]{Rakhlin2012MakingGD}
Alexander Rakhlin, Ohad Shamir, and Karthik Sridharan.
\newblock Making gradient descent optimal for strongly convex stochastic
  optimization.
\newblock \emph{ArXiv}, abs/1109.5647, 2012.

\bibitem[Bertsekas and Tsitsiklis(1995)]{Bertsekas1995NeurodynamicPA}
Dimitri~P. Bertsekas and John~N. Tsitsiklis.
\newblock Neuro-dynamic programming: an overview.
\newblock \emph{Proceedings of 1995 34th IEEE Conference on Decision and
  Control}, 1:\penalty0 560--564 vol.1, 1995.

\bibitem[Bottou et~al.(2016)Bottou, Curtis, and
  Nocedal]{https://doi.org/10.48550/arxiv.1606.04838}
Léon Bottou, Frank~E. Curtis, and Jorge Nocedal.
\newblock Optimization methods for large-scale machine learning, 2016.
\newblock URL \url{https://arxiv.org/abs/1606.04838}.

\bibitem[Schmidt et~al.(2017)Schmidt, Roux, and Bach]{Schmidt2017MinimizingFS}
Mark~W. Schmidt, Nicolas~Le Roux, and Francis~R. Bach.
\newblock Minimizing finite sums with the stochastic average gradient.
\newblock \emph{Mathematical Programming}, 162:\penalty0 83--112, 2017.

\bibitem[Zhou et~al.(2019)Zhou, Yang, Zhang, Liang, and Tarokh]{Zhou2019SGDCT}
Yi~Zhou, Junjie Yang, Huishuai Zhang, Yingbin Liang, and Vahid Tarokh.
\newblock Sgd converges to global minimum in deep learning via star-convex
  path.
\newblock \emph{ArXiv}, abs/1901.00451, 2019.

\bibitem[Karimi et~al.(2016)Karimi, Nutini, and Schmidt]{Karimi2016LinearCO}
H.~Karimi, Julie Nutini, and Mark~W. Schmidt.
\newblock Linear convergence of gradient and proximal-gradient methods under
  the polyak-Łojasiewicz condition.
\newblock In \emph{ECML/PKDD}, 2016.

\bibitem[Khaled and Richtárik()]{https://doi.org/10.48550/arxiv.2002.03329}
Ahmed Khaled and Peter Richtárik.
\newblock URL \url{https://arxiv.org/abs/2002.03329}.

\bibitem[Liu et~al.(2020)Liu, Gao, and Yin]{NEURIPS2020_d3f5d4de}
Yanli Liu, Yuan Gao, and Wotao Yin.
\newblock An improved analysis of stochastic gradient descent with momentum.
\newblock In H.~Larochelle, M.~Ranzato, R.~Hadsell, M.F. Balcan, and H.~Lin,
  editors, \emph{Advances in Neural Information Processing Systems}, volume~33,
  pages 18261--18271. Curran Associates, Inc., 2020.
\newblock URL
  \url{https://proceedings.neurips.cc/paper/2020/file/d3f5d4de09ea19461dab00590df91e4f-Paper.pdf}.

\bibitem[Yu et~al.(2019)Yu, Jin, and
  Yang]{https://doi.org/10.48550/arxiv.1905.03817}
Hao Yu, Rong Jin, and Sen Yang.
\newblock On the linear speedup analysis of communication efficient momentum
  sgd for distributed non-convex optimization, 2019.
\newblock URL \url{https://arxiv.org/abs/1905.03817}.

\bibitem[Yan et~al.(2018)Yan, Yang, Li, Lin, and
  Yang]{https://doi.org/10.48550/arxiv.1808.10396}
Yan Yan, Tianbao Yang, Zhe Li, Qihang Lin, and Yi~Yang.
\newblock A unified analysis of stochastic momentum methods for deep learning.
\newblock 2018.
\newblock \doi{10.48550/ARXIV.1808.10396}.
\newblock URL \url{https://arxiv.org/abs/1808.10396}.

\bibitem[Shi et~al.(2021)Shi, Li, Hong, and Sun]{shi2021rmsprop}
Naichen Shi, Dawei Li, Mingyi Hong, and Ruoyu Sun.
\newblock {RMS}prop converges with proper hyper-parameter.
\newblock In \emph{International Conference on Learning Representations}, 2021.
\newblock URL \url{https://openreview.net/forum?id=3UDSdyIcBDA}.

\bibitem[Défossez et~al.(2020)Défossez, Bottou, Bach, and
  Usunier]{https://doi.org/10.48550/arxiv.2003.02395}
Alexandre Défossez, Léon Bottou, Francis Bach, and Nicolas Usunier.
\newblock A simple convergence proof of adam and adagrad, 2020.
\newblock URL \url{https://arxiv.org/abs/2003.02395}.

\bibitem[De et~al.(2018)De, Mukherjee, and
  Ullah]{https://doi.org/10.48550/arxiv.1807.06766}
Soham De, Anirbit Mukherjee, and Enayat Ullah.
\newblock Convergence guarantees for rmsprop and adam in non-convex
  optimization and an empirical comparison to nesterov acceleration, 2018.
\newblock URL \url{https://arxiv.org/abs/1807.06766}.

\bibitem[Reddi et~al.(2019)Reddi, Kale, and
  Kumar]{https://doi.org/10.48550/arxiv.1904.09237}
Sashank~J. Reddi, Satyen Kale, and Sanjiv Kumar.
\newblock On the convergence of adam and beyond, 2019.
\newblock URL \url{https://arxiv.org/abs/1904.09237}.

\bibitem[Huang et~al.()Huang, Li, and
  Huang]{https://doi.org/10.48550/arxiv.2106.08208}
Feihu Huang, Junyi Li, and Heng Huang.
\newblock URL \url{https://arxiv.org/abs/2106.08208}.

\bibitem[Cutkosky and Orabona()]{https://doi.org/10.48550/arxiv.1905.10018}
Ashok Cutkosky and Francesco Orabona.
\newblock URL \url{https://arxiv.org/abs/1905.10018}.

\bibitem[Tran-Dinh et~al.(2019)Tran-Dinh, Pham, Phan, and
  Nguyen]{https://doi.org/10.48550/arxiv.1905.05920}
Quoc Tran-Dinh, Nhan~H. Pham, Dzung~T. Phan, and Lam~M. Nguyen.
\newblock Hybrid stochastic gradient descent algorithms for stochastic
  nonconvex optimization, 2019.
\newblock URL \url{https://arxiv.org/abs/1905.05920}.

\bibitem[Kingma and Ba(2014)]{https://doi.org/10.48550/arxiv.1412.6980}
Diederik~P. Kingma and Jimmy Ba.
\newblock Adam: A method for stochastic optimization, 2014.
\newblock URL \url{https://arxiv.org/abs/1412.6980}.

\bibitem[Zhuang et~al.(2020)Zhuang, Tang, Ding, Tatikonda, Dvornek,
  Papademetris, and Duncan]{https://doi.org/10.48550/arxiv.2010.07468}
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar Tatikonda, Nicha Dvornek,
  Xenophon Papademetris, and James~S. Duncan.
\newblock Adabelief optimizer: Adapting stepsizes by the belief in observed
  gradients.
\newblock 2020.
\newblock \doi{10.48550/ARXIV.2010.07468}.
\newblock URL \url{https://arxiv.org/abs/2010.07468}.

\bibitem[ruinan Jin et~al.(2022)ruinan Jin, Xing, and He]{jin2022on}
ruinan Jin, Yu~Xing, and Xingkang He.
\newblock On the convergence of m{SGD} and adagrad for stochastic optimization.
\newblock In \emph{International Conference on Learning Representations}, 2022.
\newblock URL \url{https://openreview.net/forum?id=g5tANwND04i}.

\bibitem[Faw et~al.(2022)Faw, Tziotis, Caramanis, Mokhtari, Shakkottai, and
  Ward]{https://doi.org/10.48550/arxiv.2202.05791}
Matthew Faw, Isidoros Tziotis, Constantine Caramanis, Aryan Mokhtari, Sanjay
  Shakkottai, and Rachel Ward.
\newblock The power of adaptivity in sgd: Self-tuning step sizes with unbounded
  gradients and affine variance, 2022.
\newblock URL \url{https://arxiv.org/abs/2202.05791}.

\end{thebibliography}
