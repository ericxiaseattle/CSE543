\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
%\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}    % inserting images
\bibliographystyle{unsrtnat}

\title{Convergence of Optimizers Without Bounded Gradient Assumption}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Roger Wang, \ Eric Xia \\
  \texttt{rogerwyf@uw.edu, ericxia@uw.edu} \\
}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={red!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}
\begin{document}

\maketitle

%\begin{abstract}
%\end{abstract}

\section{Introduction}

The primary goal of most deep learning models is to minimize the loss function.
Optimizers are crucial for the task of updating model weights such that the model will actually converge to a minimum in a computationally efficient manner instead of overshooting or moving away from the minimum. While existing optimizers are intuitively straightforward in convex learning, in non-convex settings they (notably for Adam-type adaptive gradient methods)
often require the assumption on the boundedness of gradients for achieving convergence.

For example, for an objective function  $f(x)$ satisfying
Polyak-Łojasiewicz conditions \cite{POLYAK1963864},  convergence rates of $O(1/T)$ were established for Stochastic Gradient Descent (SGD) under the assumption that $\mathbb{E}[||\nabla f_i(x_k)||^2] \leq C^2$ for all $x_k$ and some $C$, where $f_i$ typically represents the fit on an individual training sample \cite{DBLP:journals/corr/KarimiNS16}.

While convenient, the imposition of assumptions on the boundedness of gradients can be difficult to verify in practical settings, hence in recent years there has been a trend in research efforts towards analyzing existing optimization methods and proposing novel methods without such assumptions. 

In this report, we will survey and analyze results from five latest research papers on the convergence of optimization methods including SGD (\hyperref[section3]{Section 3}), SGD with Momentum (\hyperref[section4]{Section 4}), Adaptive Gradient (AdaGrad) (\hyperref[section5]{Section 5}), Root Mean Squared Propagation (RMSProp) (\hyperref[section6]{Section 6}) and Adaptive Moment Estimation (Adam) (\hyperref[section7]{Section 7}), comparing and contrasting their approaches, methodologies, and findings as well as discussing their broader implications for the convergence problem and the general field of optimizers.
\section{General Assumptions}
In a stochastic setting, the optimization problem for a neural network training process can be written as a finite-sum problem:
\[
\min_{x \in \mathbb{R}^d} f(x) = \frac{1}{n}\sum_{j = 0}^{n - 1} f(x, s_j)
\]
where $f(x, s_j)$ represents the loss function contributed by the randomly shuffled sample batch $s_j$.\\
\newline
Let $g_t\coloneqq\nabla f(x_t)$ and $\tilde{g}_t \coloneqq  \nabla f(x_t, s_t)$ be the full gradient and the stochastic gradient with respect to the sampled batch $s_t$ of the objective function $f$ at time or iteration $t$, respectively. Below we list the common assumptions in standard stochastic optimization that are either explicitly stated or implied in the papers covered in this report:
\begin{enumerate}[leftmargin=*]
	\item \textit{\textbf{Lower boundedness}: $f$ is lower-bounded by some function $f^*$.}
	\item \textit{\textbf{Smoothness}: $f$ is $L$-smooth or $\nabla f$ is $L$-Lipschitz continuous for some constant $L$.}
	\item \textit{\textbf{Unbiased gradients}: $\forall t, \mathbb{E}_{s_t}[\tilde{g}_t] = g_t$}.
	\item \textit{\textbf{Independence}: the random samples $s_j$'s are independent.}
	\item \textit{\textbf{Bounded variance}: $Var_{s_t}(\tilde{g}_t) = \mathbb{E}_{s_t}[\tilde{g}_t - g_t] \leq \sigma^2$ for some $\sigma > 0$.}
\end{enumerate}
Furthermore, each paper in this report may have additional assumptions which will be included in their corresponding section. It is worth mentioning that none of these assumptions is related to the boundedness to gradient $g_t$, which is often the key assumption utilized by previous theoretical work.
\section{Stochastic Gradient Descent for Structured Nonconvex Functions}
\label{section3}
\subsection{Previous research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Result}
\subsection{Key Approaches \& Insights}
\subsection{Discussion}
\newpage
\section{Stochastic Gradient Descent with Momentum}
\label{section4}
\subsection{Previous research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Result}
\subsection{Key Approaches \& Insights}
\subsection{Discussion}
\newpage
\section{AdaGrad}
\label{section5}
\subsection{Previous research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Result}
\subsection{Key Approaches \& Insights}
\subsection{Discussion}
\newpage
\section{RMSProp}
\label{section6}
\subsection{Previous research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Result}
\subsection{Key Approaches \& Insights}
\subsection{Discussion}
\newpage
\section{Adam}
\label{section7}
\subsection{Previous research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Result}
\subsection{Key Approaches \& Insights}
\subsection{Discussion}
\newpage
\section{Addition Dicussion \& Comments}
\section{Future Research}
\newpage
\bibliography{final}

%\appendix
%\section{Appendix}
%
%Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
%This section will often be part of the supplemental material.

\end{document}
