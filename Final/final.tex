\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
%\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}    % inserting images
\bibliographystyle{unsrtnat}

\title{Convergence of Optimizers Without Bounded Gradient Assumption}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Roger Wang \ \ \ \  Eric Xia \\
  University of Washington\\
  \texttt{\{rogerwyf, ericxia\}@uw.edu} 
}
\usepackage{enumitem}
\usepackage{mathtools}
\usepackage{hyperref}
\usepackage{xcolor}
\hypersetup{
	colorlinks,
	linkcolor={black!50!black},
	citecolor={blue!50!black},
	urlcolor={blue!80!black}
}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{corollary}{Corollary}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{definition}{Definition}[section]

\begin{document}

\maketitle

%\begin{abstract}
%\end{abstract}

\section{Introduction}

The primary goal of most deep learning models is to minimize the underlying loss function.
Optimizers are crucial for the task of updating model weights such that the model will actually converge to a minimum in a computationally efficient manner instead of overshooting or moving away from the minimum. While existing optimizers are intuitively straightforward in convex learning, in non-convex settings they (notably for Adam-type adaptive gradient methods)
often require the assumption on the boundedness of gradients for achieving convergence.

While convenient, the imposition of assumptions on the boundedness of gradients can be difficult to verify in practical settings, hence in recent years there has been a trend in research efforts towards analyzing existing optimization methods and proposing novel methods without such assumptions. 

In this report, we will survey and analyze results from four recent research papers on the convergence of stochastic optimization algorithms including SGD (\hyperref[section3]{Section 3}), SGD with Momentum (\hyperref[section4]{Section 4}), Root Mean Squared Propagation (RMSProp) (\hyperref[section6]{Section 5}) and Adaptive Moment Estimation (Adam) (\hyperref[section7]{Section 6}), comparing and contrasting their approaches, methodologies, and findings as well as discussing their broader implications for the convergence problem and the general field of optimizers.
\section{General Assumptions}
In a stochastic setting, the optimization problem for a neural network training process can be written as a finite-sum problem:
\[
\min_{x \in \mathbb{R}^d} f(x) = \frac{1}{n}\sum_{j = 0}^{n - 1} f(x, s_j)
\]
where $f(x, s_j)$ represents the loss function contributed by the randomly shuffled sample batch $s_j$.\\
\newline
Let $g_t\coloneqq\nabla f(x_t)$ and $\tilde{g}_t \coloneqq  \nabla f(x_t, s_t)$ be the full gradient and the stochastic gradient with respect to the sampled batch $s_t$ of the objective function $f$ at time or iteration $t$, respectively. Below we list the common assumptions in standard stochastic optimization that are either explicitly stated or implied in the papers covered in this report:
\begin{enumerate}[leftmargin=*]
	\item \textit{\textbf{Lower boundedness}: $f$ is lower-bounded by some function $f^*$.}
	\item \textit{\textbf{Smoothness}: $f$ is $L$-smooth or $\nabla f$ is $L$-Lipschitz continuous for some constant $L$.}
	\item \textit{\textbf{Unbiased gradients}: $\forall t, \mathbb{E}_{s_t}[\tilde{g}_t] = g_t$}.
	\item \textit{\textbf{Independence}: the random samples $s_j$'s are independent.}
	\item \textit{\textbf{Bounded variance}: $Var_{s_t}(\tilde{g}_t) = \mathbb{E}_{s_t}[\tilde{g}_t - g_t] \leq \sigma^2$ for some $\sigma > 0$.}
\end{enumerate}
\textbf{Throughout this report, we will refer to the above as \textit{General Assumptions} and use the above notations}. Furthermore, each paper in this report may have additional assumptions which will be included in their corresponding section. It is worth mentioning that none of these assumptions is related to the boundedness to gradient $g_t$ or $\tilde{g}_t$, which is often the key assumption utilized by previous theoretical work.
\section{Stochastic Gradient Descent for Structured Nonconvex Functions}
\label{section3}
\subsection{Previous Research \& Motivation}

Prior to~\cite{https://doi.org/10.48550/arxiv.2006.10311}, standard convergence theory for SGD in smooth non-convex settings gave a slow sublinear convergence to a stationary point. There has recently been a great interest
in exploiting additional structure of classes of nonconvex function, such as error bound properties~\cite{Fabian2010ErrorBN},
quasi (strong) convexity~\cite{https://doi.org/10.48550/arxiv.1906.11985, JMLR:v19:16-465, https://doi.org/10.48550/arxiv.1710.00797}, and quadratic growth condition~\cite{doi:10.1137/S1052623499359178}.

In~\cite{https://doi.org/10.48550/arxiv.2006.10311}, Gower et al. provide a new general analsysis of SGD focusing on the two weakest of these properties, quasar (strongly) convex (QC) functions and functions satisfying the
Polyak-≈Åojasiewicz (PL) conditions~\cite{POLYAK1963864}, using the expected residual (ER) condition~\cite{Gower2021StochasticQM}.

\subsection{Additional Assumptions}

On top of \textit{General Assumptions}, the paper makes the mild assumption that the \textit{gradient noise} $\sigma^2$ is finite:

\begin{equation}
	\sigma^2 \coloneqq \sup_{x^*\in\mathcal{X^*}} \mathbb{E}\left[||g(x^*)||^2\right] < \infty.
\end{equation}

\subsection{Main Results \& Methodology}

The main results of the paper are convergence bounds using the ER condition for SGD on QC functions and minibatch SGD on PL functions.

\begin{definition}[Expected Residual]
	We say $g\in\text{ER}(\rho)$ if

	\[
		\mathbb{E}\left[||g(x) - g(x^*) - (\nabla f(x) - \nabla f(x^*))||^2\right] \leq 2\rho (f(x) - f(x^*)), \ \ \forall x\in\mathbb{R}^d.
	\]
\end{definition}

\begin{theorem}[QC Functions with Constant and Decreasing Step-sizes]
	Assume \textit{General Assumptions}, $f(x)$ is $\zeta$-QC with respect to $x^*$, and $g\in\text{ER}(\rho)$. Let $0 < \gamma_k < \frac{\zeta}{2\rho + L}$ for all $k\in\mathbb{N}$ and let $r_0\coloneqq ||x^0 - x^*||^2$.
	Then iterates of SGD satisfy

	\[
		\substack{\min \\ t=0,\dots, k-1}\mathbb{E}\left[f(x^t) - f(x^*)\right] \leq \frac{1}{\sum_{i=0}^{k-1}}\gamma_i (\zeta - \gamma_i (2\rho + L)) \left[\frac{r^0}{2} + \sigma^2 \sum^{k-1}_{t=0}\gamma_t^2\right].
	\]

	Furthermore, for $\gamma < \frac{\zeta}{2\rho + L}$, we have that

	\textbf{1}. If $\forall k\in\mathbb{N}$, $\gamma_k = \gamma \equiv \frac{1}{2}\frac{\zeta}{(2\rho + L)}$ then $\forall k\in\mathbb{N}$,

	\[
		\substack{\min \\ t=0,\dots, k-1}\mathbb{E}[f(x^t) - f(x^*)]\leq 2r_0 \frac{2\rho + L}{\zeta^2 k} + \frac{\sigma^2}{2\rho + L}.
	\]

	\textbf{2}. Suppose SGD is run for $T$ iterations. If $\gamma_k = \frac{\gamma}{\sqrt{T}}$ for all $k$ from 0 to $T-1$, then

	\[
		\substack{\min \\ t=0,\dots, T-1} \mathbb{E}[f(x^t) - f(x^*)] \leq \frac{r_0 + 2\gamma^2 \sigma^2}{\gamma\sqrt{T}}.
	\]

	\textbf{3}. If $\forall k\in\mathbb{N},\ \gamma_k = \frac{\gamma}{\sqrt{k+1}}$ then $\forall k \in\mathbb{N}$,

	\[
		\substack{\min \\ t=0,\dots, k-1} \mathbb{E}[f(x^t) - f(x^*)] \leq \frac{1}{4\gamma} \frac{r_0 + 2\gamma^2 \sigma^2 (\log{(k)} + 1)}{\zeta (\sqrt{k} - 1) - \gamma(\rho + L/2)(\log{(k)} + 1)},
	\]

	which converges at a rate $\mathcal{O}\left(\frac{\log(k)}{\sqrt{k}}\right)$.
\end{theorem}

\begin{theorem}[PL Functions with Constant Step-sizes]
	Assume \textit{General Assumptions}, $f\in\text{PL}(\mu)$, and $g\in\text{ER}(\rho)$. Let $\gamma_k = \gamma\leq \frac{1}{1+2\rho/\mu}\frac{1}{L}$, for all $k$, then SGD converges as follows:

	\[
		\mathbb{E}[f(x^k) - f^*] \leq (1-\gamma\mu)^k[f(x^0) - f^*] + \frac{L\gamma\sigma^2}{\mu}.
	\]

	Thus, given $\epsilon > 0$ and using step size $\gamma = \frac{1}{L}\min\{\frac{\mu\epsilon}{2\sigma^2}, \frac{1}{1+2\rho/\mu}\}$ we have that

	\[
		k\geq \frac{L}{\mu}\max\{\frac{2\sigma^2}{\mu\epsilon}, 1+\frac{2\rho}{\mu}\}\log\left(\frac{2(f(x^0) - f^*)}{\epsilon}\right) \implies \mathbb{E}[f(x^k) - f^*]\leq \epsilon.
	\]

	When the function interpolates the data, SGD converges to the solution at a linear rate.
\end{theorem}

\subsection{Discussion}
\section{Stochastic Gradient Descent with Momentum}
\label{section4}
\subsection{Previous Research \& Motivation}
\setcounter{equation}{0}
Prior to~\cite{NEURIPS2020_d3f5d4de} by Liu et al., there had been some interests in investigating the convergence of SGDM.~\cite{https://doi.org/10.48550/arxiv.1905.03817} provides a global convergence of SGDM but it assumes uniformly boundedness of gradients of the objection funcion.~\cite{https://doi.org/10.48550/arxiv.1808.10396} presents a convergence bound of SGDM for general nonconvex functions but does not explain the competitiveness of SGDM compared to SGD.
Moreover, the convergence rate of Multistage SGDM had not been established except for the classic SGD case.

In this work, Liu et al. provide a novel convergence analysis for SGDM and
Multistage\footnote{Multistage refers to applying a constant stepsize which is then dropped by a constant factor to encourage fine-tuning of training, and the momentum weight is either kept unchanged or gradually increased.}
SGDM without bounded gradient assumptions. This work also demonstrates that SGDM has the same convergence bound as SGD for both strongly convex and nonconvex functions without uniformly bounded gradient assumption,
and is the first convergence guarantee for SGDM in a multistage setting.

\subsection{Main Results \& Methodology}
The main results of this paper are the convergence bounds of SGDM and Multistage SGDM. In SGDM, let $\alpha$ and $\beta$ be learning rate and momentum weight, we have the following result:
\begin{theorem}[Non-convex SGDM]
	\label{theom41} Assume $f:\mathbb{R}^d \rightarrow \mathbb{R}$ satisfies General Assumptions, let $\alpha \leq \min\{\frac{1 - \beta}{L(4 - \beta + \beta^2)}, \frac{1 - \beta}{2\sqrt{2}L\sqrt{\beta + \beta^2}}\}$, then
	\[
		\frac{1}{k}\sum_{i = 1}^{k}\mathbb{E}[\|g_t\|^2] \leq \frac{2 (f(x_1) - f^*}{k\alpha} + (\frac{\beta + 3\beta^2}{2 (1 + \beta)} + 1)L\alpha\sigma^2 = \mathcal{O}(\frac{f(x_1) - f^*}{k\alpha} + L\alpha\sigma^2)
	\]
\end{theorem}
\begin{theorem}[Strongly Convex SGDM]
	\label{theom42} Assume $f:\mathbb{R}^d \rightarrow \mathbb{R}$ satisfies General Assumptions and is $\mu$-strongly convex, let $\alpha \leq \min\{\frac{1 - \beta}{5L}, \frac{1 - \beta}{L(3 - \beta + 2\beta^2 + \frac{48\sqrt{\beta}}{25}\frac{2L + 18\mu}{L})}\}$, then for all $t \geq t_0:= \lfloor\frac{\log 0.5}{\log \beta}\rfloor$, 
	\[
		\mathbb{E}[f(x_t) - f^*] = \mathcal{O}\bigl(\max\{1 - \alpha\mu, \beta\} + \frac{L}{\mu}\alpha\sigma^2\bigr)
	\]
\end{theorem}
In a multistage setting, let $\alpha_i$, $\beta_i$ and $T_i$ are learning rate (step size), momentum weight and stage length of $i$th stage, respectively, we have the following result:
\begin{theorem}[Non-convex Multistage SGDM]
	\label{theom43} Assume $f:\mathbb{R}^d \rightarrow \mathbb{R}$ satisfies General Assumptions, restrict the parameters in each stage of Multistage SGDM so that 
\begin{equation}
\label{eq41}
\begin{split}
\frac{\alpha_i\beta_i}{1 - \beta_i} &\equiv A_1 \ \ \ \ i = 1,...,n\\
\alpha_iT_i &\equiv A_2 \ \ \ \ i = 1,...,n\\
0 \leq \beta_1 &\leq \beta_2 \leq ... \leq \beta_n \leq 1
\end{split}
\end{equation}
and $A_1, A_2$ are properly chosen constants. Let $A_1 = \frac{1}{24\sqrt{2}L}$ and $A_2$ be large enough so that $\beta_i^{2T_i} \leq \frac{1}{2}$ for $i = 1,...,n$. In addition, let
\[
\frac{1 - \beta_1}{\beta_1} \leq 12 \frac{1 - \beta_n}{\sqrt{\beta_n + \beta_n^2}}
\] then we have
\[
\begin{split}
\frac{1}{n}\sum_{l = 1}^{n}\frac{1}{T_l}\sum_{i = T_1 + ... + T_{l - 1} + 1}^{T_1 + .. + T_l}\mathbb{E}[\|g_t\|^2] &\leq \frac{2(f(x_1) - f^*)}{nA_2} + \frac{1}{n}\sum_{i = 1}^n\Bigl(24\beta^2_l\frac{\beta_1}{\sqrt{\beta_n + \beta_n^2}L + 3L}\Bigr)\alpha_l\sigma^2\\
&= \mathcal{O}\Bigl(\frac{2(f(x_1) - f^*)}{nA_2} + \frac{1}{n}\sum_{i = 1}^nL\alpha_l\sigma^2\Bigr)
\end{split}
\]
\end{theorem}
Next, we will summarize the key approaches used to derive the above results. Recall that the core of SGDM algorithm is the following updating rule:
\[
v_t = \beta v_{t - 1} + (1 - \beta)\tilde{g}_t \ \ \ \
x_{t + 1} = x_t - \alpha v_t
\]
Therefore, assume $v_0 = 0$, then $v_t$ can be expressed as 
\begin{equation}
\label{eq42}
v_t = (1 - \beta)\sum_{i = 1}^{t}\beta^{k - i}\tilde{g}_i
\end{equation}
One key observation on the role of $\beta$ in equation (\hyperref[eq42]{2}) from the paper is that $v_t$ enjoys a reduced variance of $(1 - \beta)\sigma^2$ while having a controllable deviation from the full gradient $g_t$ in expectation since $v_t$ is a moving average of the past stochastic gradients with lower weights on the older ones, thus it makes sense to look at the deterministic version of $v_t$ (replacing $\tilde{g}_i$ with $g_i$) and its deviation from the ideal descent direction $g_t$, which could be unbounded without further assumptions.

While previous work assumed the boundedness of $g_t$ to circumvent above difficulty, this work constructed a novel Lyapunov function to handle this deviation:
\[
L_t = (f(z_t) - f^*) + \sum_{i = 1}^{k - 1}c_i\|x^{k + 1 - i} - x^{k - i}\|^2
\]
where $z_t = \begin{cases}
	x_t & t = 1 \\
	\frac{1}{1 - \beta}x_t - \frac{\beta}{1 - \beta}x_{t - 1} & t \geq 2
\end{cases}$\\

The authors then argued that by carefully defining $\{c_i\}_i^\infty$ such that it is a positive sequence in a diminishing fashion, $L_t$ is indeed a Lyapunov function, thus one can show that $\mathbb{E}[L_{t + 1} - L_t] \leq -R_1E[\|g_t\|^2] + R_2$ for some positive constants $R_1 \geq \frac{\alpha}{2}$ and $R_2 = \mathcal{O}(L\alpha\sigma^2)$. By telescoping this inequality, the convergence of SGDM in \hyperref[theom41]{Theorem 4.1} is then obtained, and similar techniques were utilized to derive the results in \hyperref[theom42]{Theorem 4.2} under a strongly convex setting and in \hyperref[theom43]{Theorem 4.3} for Multistage SGDM.
\subsection{Discussion}
\hyperref[theom41]{Theorem 4.1} and \hyperref[theom42]{Theorem 4.2} show that under both nonconvex and strongly convex settings, with a proper learning rate $\alpha$, SGDM can achieve the same convergence bound as the classical convergence bound of SGD (as shown in previous work, e.g., Theorem 4.5 and 4.8 in~\cite{https://doi.org/10.48550/arxiv.1606.04838}). This result only depends on General Assumptions, and the radius of the stationary distribution is smaller than the previous $\mathcal{O}(\frac{\alpha\sigma^2}{1 - \beta})$ result from~\cite{https://doi.org/10.48550/arxiv.1905.03817} that relies on the additional assumption of uniformly bounded gradients. It is also worth mentioning that the use of Lyapunov function is a novel approach for convergence analysis of optimization algorithms and provides some new insights throughout this paper. 

For Multistage SGDM, \hyperref[theom42]{Theorem 4.3} is the first theoretical result that guarantees its convergence. Moreover, it was demonstrated from the convergence bound that large learning rates are allowed in the first a few stages to accelerate the initial convergence, and smaller learning rates can refine the radius of the stationary distribution in the later stages, which is an advantage of stagewise training compared to plain SGDM.

However, the convergence analysis in this paper does have some weaknesses and limitations: First of all, although it is theoretically shown that SGDM is "at least as fast as" SGD, this paper did not explore the advantages of SGDM compared to SGD in detail. In addition, \hyperref[theom42]{Theorem 4.2} assumes a lower-bound of timestamp/iteration for the result to be valid, and this lower-bound could be a problem for some choices of $\beta$ (e.g, when $\beta = 0.995$, $t_0 = 138$). Finally, equation (\hyperref[eq41]{1}) from \hyperref[theom43]{Theorem 4.3} puts a strong restriction on the choice of learning rates and momentum weights at all stages, which makes this stage-wise training setup impractical.
\section{RMSProp}
\label{section6}
\setcounter{equation}{0}
\subsection{Previous Research \& Motivation}
Prior to~\cite{shi2021rmsprop} by Shi et al., there had been one line of research on the convergence of variants of Adam (which includes RMSProp) with additional assumptions.~\cite{https://doi.org/10.48550/arxiv.2003.02395} provides a clean convergence result but assumed a large $\epsilon$ compared to  weighted moving average of the squared gradient, which is in contrary to the spirit of RMSProp.~\cite{https://doi.org/10.48550/arxiv.1807.06766} analyzes deterministic and stochastic RMSprop, but their results were based on an rather unrealistic assumption that all stochastic gradients have the same sign. Furthermore, all the above mentioned works assume the gradients to be bounded.

During the review on one counter-example to the convergence of Adam from~\cite{https://doi.org/10.48550/arxiv.1904.09237}, Shi et al. ran simulations and found that there is always a threshold of the moving average parameter above which
RMSProp converges, thus motivating them to further investigate the relationship between this parameter and the performance of the algorithm. They discovered that the convergence of RMSProp algorithm is
contingent to the choice of the moving average parameter. Moreover, they proved that RMSProp converges to stationary points for certain types of problems and to bounded region for the others, which was the first result of convergence
of this algorithm with no assumption about the boundedness of the gradient norm.

\subsection{Additional Assumptions}
In addition to \textit{General Assumptions}, this paper assumes that for stochastic RMSProp, the objective satisfies
\begin{equation}
\label{eq51}
\sum_{j = 0}^{n - 1} \|\nabla f_j(x)\|_2^2 \leq D_1 \|\nabla f(x)\|_2^2 + D_0
\end{equation}
for some non-negative constant $D_0$ and $D_1$. This can be viewed as an augment to the bounded variance assumption.
\subsection{Main Results \& Methodology}
The main results of this paper are the convergence of RMSProp under both deterministic and stochastic setting. Let $\alpha_t$  be the learning rate at time/iteration $t$ and $\beta$ be the moving average parameter of the squared gradients norm, we have the following results: 
\begin{theorem}[Deterministic RMSProp]
	\label{theom51}
Assume $f:\mathbb{R}^d \rightarrow \mathbb{R}$ satisfies General Assumptions, then for deterministic RMSProp (i.e, full-batch with $n = 1$, $\epsilon = 0$) with a diminishing learning rate $\alpha_t = \frac{\alpha_1}{\sqrt{t}}$ and any $\beta \in (0, 1)$, we have
\[
\min_{t \in (1, T]} \|g_t\|_1 \leq \mathcal{O}\Big(\frac{\log T}{\sqrt{T}}\Big)
\]
\end{theorem}
where $T > 0$ is the total number of iterations.
\begin{theorem}[Stochastic RMSProp - Bounded Region]
	\label{theom52}
	Assume $f:\mathbb{R}^d \rightarrow \mathbb{R}$ satisfies General Assumptions and (\hyperref[eq51]{1}). In addition, assume $\beta$ satisfies
	\[
	\sqrt{\frac{10dn}{\beta^n}}dnD_1\Big((1 - \beta)\frac{\frac{4n^2}{\beta^n} - 1}{2} + (\frac{1}{\sqrt{\beta^n}}- 1)\Big)\leq \frac{\sqrt{2} - 1}{2\sqrt{2}}
	\]
	Then, for stochastic RMSProp with a diminishing learning rate $\alpha_t = \frac{\alpha_1}{\sqrt{t}}$, we have
	\[
	\min_{t \in (1, T]} \min\{\|\tilde{g}_t\|_1, \|\tilde{g}_t\|_2^2\sqrt{\frac{D_1d}{D_0}}\} \leq \mathcal{O}\Big(\frac{\log T}{\sqrt{T}}\Big) + \mathcal{O}\Big(C\sqrt{D_0}\Big), \ \ \ \forall \ T\geq 4
	\]
	where $C$ is a $\beta$-dependent constant that satisfies $\lim\limits_{\beta \rightarrow 1} C = 0$
\end{theorem}
\begin{corollary}[Stochastic RMSProp - Stationary Point]
	\label{coro51}
	Let all assumptions in Theorem 5.2 hold. In addition, assume $D_0 = 0$ in (\hyperref[eq51]{1}), then for stochastic RMSProp we have,
\[
\min_{t \in (1, T]} \|\tilde{g}_t\|_1 \leq \mathcal{O}\Big(\frac{\log T}{\sqrt{T}}\Big), \ \ \ \forall \ T \geq 4
\]
\end{corollary}
Next, we explain the methodology used to derive the above results. Recall that the core of the RMSProp algorithm is the following updating rule:
\[
v_t = \beta v_{t - 1} + (1 - \beta)(g_t \circ g_t)\ \  \ \ \ x_t = x_{t - 1} - \alpha(v_t + \epsilon I)^{-1/2}g_t
\] 
For the full-batch version of RMSProp, \hyperref[theom51]{Theorem 5.1} is an extension of results from the original Adam paper~\cite{Gower2021StochasticQM} for which the authors utilized the Lipschitz continuity of the gradients to remove the bounded assumption.\\
\newline
The derivation to the results for stochastic RMSProp is more complicated: Based on the observation from the simulation, the authors divided stochastic optimization problems into two classes: \textit{realizable} problems when $D_0 = 0$ from equation (\hyperref[eq51]{1}) and  \textit{non-realizable} problems when $D_0 \neq 0$. They further conjectured that these two classes of problems correspond to different convergence behavior of RMSProp contingent to the choice of beta, which is summarized in Table \hyperref[tb1]{1}:
\begin{table}[h]
\label{tb1}
\centering
\begin{tabular}{c|c | c}
	\hline
	Class & $\beta$ close to 1 & $\beta$ close to 0\\
	\hline
	Non-realizable & Convergence to bounded region (\hyperref[theom51]{Theorem 5.2}) & Divergence\\
	Realizable & Convergence to stationary points (\hyperref[coro51]{Corollary 5.1}) & Divergence\\
	\hline
\end{tabular}
\vspace{3pt}
\caption{Conjecture on convergence outcomes of RMSProp on classes of optimization problems.}
\end{table}
\ \\
The rationale to prove this conjecture is to track the magnitude of squared gradient terms and calculate the diminishing speed of $\|g\|_1$. Since $f$ is $L$-Lipschitz, by descent lemma, one can show the following inequality
\begin{equation}
\label{eq52}
f(x_{t + 1}) - f(x_{t}) \leq \langle g_t, x_{t + 1} - x_{t}\rangle + \frac{L}{2}\|x_{t + 1} - x_{t}\|^2
\end{equation}
By examining the distribution of gradient norms among different sampled batches, one can find upper and lower bounds of the inner product between the gradient and difference in iterations of $x_t$ and those of $v_t$, which are conditioned on the choice of $\beta$. Moreover, these bounds directly contribute to bounding the first and second term in inequality  (\hyperref[eq52]{2}). Therefore, summing up the inequalities from $t_{init} = 4$ to $T$ gives the results in \hyperref[theom51]{Theorem 5.2}, and \hyperref[coro51]{Corollary 5.1} is a special case for when $D_0 = 0$.
\subsection{Discussion}
\newpage
\section{Adam}
\label{section7}
\subsection{Previous Research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Results \& Methodology}
\subsection{Discussion}
\section{Addition Discussion \& Comments}
\section{Future Research}
\pagebreak
\bibliography{final}

%\appendix
%\section{Appendix}
%
%Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
%This section will often be part of the supplemental material.

\end{document}
