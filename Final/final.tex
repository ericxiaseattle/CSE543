\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
%\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[preprint]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}    % inserting images
\bibliographystyle{unsrtnat}

\title{Convergence of Optimizers Without Bounded Gradient Assumption}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Roger Wang, \ Eric Xia \\
  \texttt{rogerwyf@uw.edu, ericxia@uw.edu} \\
}

\begin{document}

\maketitle

%\begin{abstract}
%\end{abstract}

\section{Introduction}

The primary goal of most deep learning models is to minimize the loss function.
Optimizers are crucial for the task of updating model weights such that the model will actually converge to a minimum in a computationally efficient manner instead of overshooting or moving away from the minimum. While existing optimizers are intuitively straightforward in convex learning, in non-convex settings they (notably for Adam-type adaptive gradient methods)
often require the assumption on the boundedness of gradients for achieving convergence.

For example, for an objective function  $f(x)$ satisfying
Polyak-≈Åojasiewicz conditions \cite{POLYAK1963864},  convergence rates of $O(1/T)$ were established for Stochastic Gradient Descent (SGD) under the assumption that $\mathbb{E}[||\nabla f_i(x_k)||^2] \leq C^2$ for all $x_k$ and some $C$, where $f_i$ typically represents the fit on an individual training sample \cite{DBLP:journals/corr/KarimiNS16}.

While convenient, the imposition of assumptions on the boundedness of gradients can be difficult to verify in practical settings, hence in recent years there has been a trend in research efforts towards analyzing existing optimization methods and proposing novel methods without such assumptions.

\section{General Assumptions}
\newpage
\section{Stochastic Gradient Descent for Structured Nonconvex Functions}
\subsection{Previous research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Result}
\subsection{Key Approaches \& Insights}
\subsection{Discussion}
\newpage
\section{Stochastic Gradient Descent with Momentum}
\subsection{Previous research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Result}
\subsection{Key Approaches \& Insights}
\subsection{Discussion}
\newpage
\section{AdaGrad}
\subsection{Previous research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Result}
\subsection{Key Approaches \& Insights}
\subsection{Discussion}
\newpage
\section{RMSProp}
\subsection{Previous research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Result}
\subsection{Key Approaches \& Insights}
\subsection{Discussion}
\newpage
\section{Adam}
\subsection{Previous research \& Motivation}
\subsection{Additional Assumptions}
\subsection{Main Result}
\subsection{Key Approaches \& Insights}
\subsection{Discussion}
\newpage
\section{Addition Dicussion \& Comments}
\section{Future Research}
\newpage
\bibliography{final}

%\appendix
%\section{Appendix}
%
%Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
%This section will often be part of the supplemental material.

\end{document}
