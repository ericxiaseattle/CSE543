@article{tieleman2012lecture,
  title={Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude},
  author={Tieleman, Tijmen and Hinton, Geoffrey and others},
  journal={COURSERA: Neural networks for machine learning},
  volume={4},
  number={2},
  pages={26--31},
  year={2012}
}

@article{POLYAK1963864,
title = {Gradient methods for the minimisation of functionals},
journal = {USSR Computational Mathematics and Mathematical Physics},
volume = {3},
number = {4},
pages = {864-878},
year = {1963},
issn = {0041-5553},
doi = {https://doi.org/10.1016/0041-5553(63)90382-3},
url = {https://www.sciencedirect.com/science/article/pii/0041555363903823},
author = {B.T. Polyak},
abstract = {Let tf(t) be a functional defined in the (real) Hubert space H. The problem consists in finding its minimum value tff∗ = inf tf(x) and some minimum point x∗ (if such exists).}
}

@article{DBLP:journals/corr/KarimiNS16,
  author    = {Hamed Karimi and
               Julie Nutini and
               Mark Schmidt},
  title     = {Linear Convergence of Gradient and Proximal-Gradient Methods Under
               the Polyak-{\L}ojasiewicz Condition},
  journal   = {CoRR},
  volume    = {abs/1608.04636},
  year      = {2016},
  url       = {http://arxiv.org/abs/1608.04636},
  eprinttype = {arXiv},
  eprint    = {1608.04636},
  timestamp = {Thu, 23 Apr 2020 11:53:21 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KarimiNS16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{shi2021rmsprop,
    title={{RMS}prop converges with proper hyper-parameter},
    author={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=3UDSdyIcBDA}
}

@inproceedings{NEURIPS2020_d3f5d4de,
    author = {Liu, Yanli and Gao, Yuan and Yin, Wotao},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {18261--18271},
    publisher = {Curran Associates, Inc.},
    title = {An Improved Analysis of Stochastic Gradient Descent with Momentum},
    url = {https://proceedings.neurips.cc/paper/2020/file/d3f5d4de09ea19461dab00590df91e4f-Paper.pdf},
    volume = {33},
    year = {2020}
}

@misc{https://doi.org/10.48550/arxiv.2006.10311,
    doi = {10.48550/ARXIV.2006.10311},
    url = {https://arxiv.org/abs/2006.10311},
    author = {Gower, Robert M. and Sebbouh, Othmane and Loizou, Nicolas},
    keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2106.08208,
    doi = {10.48550/ARXIV.2106.08208},
    url = {https://arxiv.org/abs/2106.08208},
    author = {Huang, Feihu and Li, Junyi and Huang, Heng},
    keywords = {Optimization and Control (math.OC), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients},
    publisher = {arXiv},
    year = {2021},
    copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2202.05791,
    doi = {10.48550/ARXIV.2202.05791},
    url = {https://arxiv.org/abs/2202.05791},
    author = {Faw, Matthew and Tziotis, Isidoros and Caramanis, Constantine and Mokhtari, Aryan and Shakkottai, Sanjay and Ward, Rachel},
    keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
    title = {The Power of Adaptivity in SGD: Self-Tuning Step Sizes with Unbounded Gradients and Affine Variance},
    publisher = {arXiv},
    year = {2022},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DBLP:journals/corr/abs-1904-09237,
    author    = {Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
    title     = {On the Convergence of Adam and Beyond},
    journal   = {CoRR},
    volume    = {abs/1904.09237},
    year      = {2019},
    url       = {http://arxiv.org/abs/1904.09237},
    eprinttype = {arXiv},
    eprint    = {1904.09237},
    timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1904-09237.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{https://doi.org/10.48550/arxiv.1412.6980,
  doi = {10.48550/ARXIV.1412.6980},
  
  url = {https://arxiv.org/abs/1412.6980},
  
  author = {Kingma, Diederik P. and Ba, Jimmy},
  
  keywords = {Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {Adam: A Method for Stochastic Optimization},
  
  publisher = {arXiv},
  
  year = {2014},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{ward2018adagrad,
    title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
    author={Rachel Ward and Xiaoxia Wu and Leon Bottou},
    year={2018},
    eprint={1806.01811},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}

@article{https://doi.org/10.48550/arxiv.1808.10396,
  doi = {10.48550/ARXIV.1808.10396},
  
  url = {https://arxiv.org/abs/1808.10396},
  
  author = {Yan, Yan and Yang, Tianbao and Li, Zhe and Lin, Qihang and Yang, Yi},
  
  keywords = {Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Unified Analysis of Stochastic Momentum Methods for Deep Learning},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1905.03817,
  doi = {10.48550/ARXIV.1905.03817},
  
  url = {https://arxiv.org/abs/1905.03817},
  
  author = {Yu, Hao and Jin, Rong and Yang, Sen},
  
  keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {On the Linear Speedup Analysis of Communication Efficient Momentum SGD for Distributed Non-Convex Optimization},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1606.04838,
  doi = {10.48550/ARXIV.1606.04838},
  
  url = {https://arxiv.org/abs/1606.04838},
  
  author = {Bottou, Léon and Curtis, Frank E. and Nocedal, Jorge},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Optimization Methods for Large-Scale Machine Learning},
  
  publisher = {arXiv},
  
  year = {2016},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2003.02395,
  doi = {10.48550/ARXIV.2003.02395},
  
  url = {https://arxiv.org/abs/2003.02395},
  
  author = {Défossez, Alexandre and Bottou, Léon and Bach, Francis and Usunier, Nicolas},
  
  keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},
  
  title = {A Simple Convergence Proof of Adam and Adagrad},
  
  publisher = {arXiv},
  
  year = {2020},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.1807.06766,
  doi = {10.48550/ARXIV.1807.06766},
  
  url = {https://arxiv.org/abs/1807.06766},
  
  author = {De, Soham and Mukherjee, Anirbit and Ullah, Enayat},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {Convergence guarantees for RMSProp and ADAM in non-convex optimization and an empirical comparison to Nesterov acceleration},
  
  publisher = {arXiv},
  
  year = {2018},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{https://doi.org/10.48550/arxiv.1904.09237,
  doi = {10.48550/ARXIV.1904.09237},
  
  url = {https://arxiv.org/abs/1904.09237},
  
  author = {Reddi, Sashank J. and Kale, Satyen and Kumar, Sanjiv},
  
  keywords = {Machine Learning (cs.LG), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
  
  title = {On the Convergence of Adam and Beyond},
  
  publisher = {arXiv},
  
  year = {2019},
  
  copyright = {arXiv.org perpetual, non-exclusive license}
}
