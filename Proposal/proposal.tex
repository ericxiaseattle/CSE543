\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2021

% ready for submission
%\usepackage{neurips_2021}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
     \usepackage[final]{neurips_2021}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2021}

% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2021}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

\usepackage{graphicx}    % inserting images
\bibliographystyle{unsrtnat}

\title{Convergence of Optimizers Without Bounded Gradient Assumption}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Yufeng (Roger) Wang, \ Eric Xia \\
  \texttt{rogerwyf@uw.edu, ericxia@uw.edu} \\
}

\begin{document}

\maketitle

%\begin{abstract}
%\end{abstract}

\section{Introduction}

The primary goal of most deep learning models is to minimize the loss function.
Optimizers are crucial for the task of updating model weights such that the model will actually converge to a minimum in a computationally efficient manner instead of overshooting or moving away from the minimum. While existing optimizers are intuitively straightforward in convex learning, in non-convex settings they (notably for Adam-type adaptive gradient methods)
often require the assumption on the boundedness of gradients for achieving convergence.

For example, for an objective function  $f(x)$ satisfying
Polyak-Łojasiewicz conditions \cite{POLYAK1963864},  convergence rates of $O(1/T)$ were established for Stochastic Gradient Descent (SGD) under the assumption that $\mathbb{E}[||\nabla f_i(x_k)||^2] \leq C^2$ for all $x_k$ and some $C$, where $f_i$ typically represents the fit on an individual training sample \cite{DBLP:journals/corr/KarimiNS16}.

While convenient, the imposition of assumptions on the boundedness of gradients can be difficult to verify in practical settings, hence in recent years there has been a trend in research efforts towards analyzing existing optimization methods and proposing novel methods without such assumptions.

\section{Proposal and Selected Papers}
We plan to survey and analyze five recent papers on convergence issues without bounded gradient assumption of optimization algorithms
including Root Mean Squared Propagation (RMSProp)~\cite{shi2021rmsprop}, SGD~\cite{https://doi.org/10.48550/arxiv.2006.10311},
SGD with momentum (SGDM)~\cite{NEURIPS2020_d3f5d4de}, Adaptive Moment Estimation (Adam)~\cite{https://doi.org/10.48550/arxiv.2106.08208}, and Adaptive Subgradient (AdaGrad)~\cite{https://doi.org/10.48550/arxiv.2202.05791},
comparing and contrasting their approaches, methodologies, and findings, and discussing their broader implications for the convergence problem and the general field of optimizers.

As a non-momentum specificity of Adam~\cite{https://doi.org/10.48550/arxiv.1412.6980}, RMSProp~\cite{tieleman2012lecture} is known for performing well empirically, despite theoretical analyses of it suggesting divergence even for simple complex functions.
In their ICRL 2021 paper, Shi et al.~\cite{shi2021rmsprop} examined a counter-example presented by Reddi et al.~\cite{DBLP:journals/corr/abs-1904-09237} in 2018 in their study of the convergence of Adam,
discovering a gap in their analysis of convergence for large $\beta_2$ parameter. They then ran simulations based on this gap. By plotting whether convergence was achieved for different choices of $\beta_2$ for RMSprop, they found that in general, there was a continuous and nontrivial curve sloping upwards, from divergence to
convergence, suggesting their conjecture that RMSprop converges for large enough $\beta_2$. This paper is notable and interesting as it is the first to prove the convergence of RMSprop
without the bounded gradient assumption.

In their AISTATS 2021 paper, Gower et al.~\cite{https://doi.org/10.48550/arxiv.2006.10311} showed that SGD converges at a rate of $\mathcal{O}(1/\sqrt{k})$
on Quasar (Strongly) Convex functions and proved linear convergence to a neighborhood for functions satisfying the Polyak-Łojasiewicz conditions without any bounded gradient assumption, instead relying on the expected residual (ER) condition for support. The significance of this study
resides in the provided insights on the complexity of minibatching and determination of optimal batch sizes, as well as the demonstration that for models interpolating the training data,
the ER condition could be discarded and still give state-of-the-art results.

\pagebreak % to avoid breaking paragraph in middle

The third paper we chose is interesting for a similar reason: it is the first convergence guarantee for Multistage SGDM without uniformly bounded gradient assumption by Liu et al.~\cite{NEURIPS2020_d3f5d4de}—Multistage refers to applying a constant stepsize
which is then dropped by a constant factor to encourage fine-tuning of training, and the momentum weight is either kept unchanged or gradually increased. Moreover, 
although SGDM has an upper hand against SGD empirically, the theoretical understanding of momentum in the stochastic case is far from complete, given that previous analyses of SGDM
either "provide worse convergence bounds than those of SGD, or assume Lipschitz or quadratic objectives, which fail to hold in practice." This work demonstrated that SGDM has the same convergence bound as SGD for both strongly convex and nonconvex functions without uniformly bounded gradient assumption.

In their NeurIPS 2021 paper, Huang et al.~\cite{https://doi.org/10.48550/arxiv.2106.08208} proposed a faster and universal framework for adaptive gradients that includes most already existing
adaptive gradient forms of optimizers. Under the current framework of adaptive gradients, the adaptive learning rate varies between individual algorithms,
e.g. Adam vs. AdaGrad-Norm~\cite{ward2018adagrad}. Not only does the proposed framework have a universal adaptive learning rate, it also guarantees
convergence without assuming boundedness of the gradient. Furthermore, they experimentally validate the outperformance of their algorithm against existing adaptive algorithms,
demonstrating the signifance of this proposition as it is both more generalizable theoretically and empirically better in performance.

Finally, last paper we look at is a very recent study from Faw et al.~\cite{https://doi.org/10.48550/arxiv.2202.05791} on convergence rates of AdaGrad-Norm\cite{ward2018adagrad}. This work holds a particular significance in their demonstration that
neither the bounded gradient assumption nor the bounded variance assumption are necessary in AdaGrad-Norm\cite{ward2018adagrad},
thus adaptive gradient methods converge in much broader situations than previously understood.

\medskip
\small
\bibliography{proposal}

%\appendix
%\section{Appendix}
%
%Optionally include extra information (complete proofs, additional experiments and plots) in the appendix.
%This section will often be part of the supplemental material.

\end{document}
