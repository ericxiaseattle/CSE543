@inproceedings{shi2021rmsprop,
    title={{RMS}prop converges with proper hyper-parameter},
    author={Naichen Shi and Dawei Li and Mingyi Hong and Ruoyu Sun},
    booktitle={International Conference on Learning Representations},
    year={2021},
    url={https://openreview.net/forum?id=3UDSdyIcBDA}
}

@inproceedings{NEURIPS2020_d3f5d4de,
    author = {Liu, Yanli and Gao, Yuan and Yin, Wotao},
    booktitle = {Advances in Neural Information Processing Systems},
    editor = {H. Larochelle and M. Ranzato and R. Hadsell and M.F. Balcan and H. Lin},
    pages = {18261--18271},
    publisher = {Curran Associates, Inc.},
    title = {An Improved Analysis of Stochastic Gradient Descent with Momentum},
    url = {https://proceedings.neurips.cc/paper/2020/file/d3f5d4de09ea19461dab00590df91e4f-Paper.pdf},
    volume = {33},
    year = {2020}
}

@misc{https://doi.org/10.48550/arxiv.2006.10311,
    doi = {10.48550/ARXIV.2006.10311},
    url = {https://arxiv.org/abs/2006.10311},
    author = {Gower, Robert M. and Sebbouh, Othmane and Loizou, Nicolas},
    keywords = {Optimization and Control (math.OC), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {SGD for Structured Nonconvex Functions: Learning Rates, Minibatching and Interpolation},
    publisher = {arXiv},
    year = {2020},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{https://doi.org/10.48550/arxiv.2106.08208,
    doi = {10.48550/ARXIV.2106.08208},
    url = {https://arxiv.org/abs/2106.08208},
    author = {Huang, Feihu and Li, Junyi and Huang, Heng},
    keywords = {Optimization and Control (math.OC), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Mathematics, FOS: Mathematics, FOS: Computer and information sciences, FOS: Computer and information sciences},
    title = {SUPER-ADAM: Faster and Universal Framework of Adaptive Gradients},
    publisher = {arXiv},
    year = {2021},
    copyright = {Creative Commons Attribution Non Commercial Share Alike 4.0 International}
}

@misc{https://doi.org/10.48550/arxiv.2202.05791,
    doi = {10.48550/ARXIV.2202.05791},
    url = {https://arxiv.org/abs/2202.05791},
    author = {Faw, Matthew and Tziotis, Isidoros and Caramanis, Constantine and Mokhtari, Aryan and Shakkottai, Sanjay and Ward, Rachel},
    keywords = {Machine Learning (stat.ML), Machine Learning (cs.LG), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},
    title = {The Power of Adaptivity in SGD: Self-Tuning Step Sizes with Unbounded Gradients and Affine Variance},
    publisher = {arXiv},
    year = {2022},
    copyright = {arXiv.org perpetual, non-exclusive license}
}

@article{DBLP:journals/corr/abs-1904-09237,
    author    = {Sashank J. Reddi and Satyen Kale and Sanjiv Kumar},
    title     = {On the Convergence of Adam and Beyond},
    journal   = {CoRR},
    volume    = {abs/1904.09237},
    year      = {2019},
    url       = {http://arxiv.org/abs/1904.09237},
    eprinttype = {arXiv},
    eprint    = {1904.09237},
    timestamp = {Fri, 26 Apr 2019 13:18:53 +0200},
    biburl    = {https://dblp.org/rec/journals/corr/abs-1904-09237.bib},
    bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{doi:10.1137/19M1263443,
    author = {Barakat, Anas and Bianchi, Pascal},
    title = {Convergence and Dynamical Behavior of the ADAM Algorithm for Nonconvex Stochastic Optimization},
    journal = {SIAM Journal on Optimization},
    volume = {31},
    number = {1},
    pages = {244-274},
    year = {2021},
    doi = {10.1137/19M1263443},
    URL = {https://doi.org/10.1137/19M1263443},
    eprint = {https://doi.org/10.1137/19M1263443},
%    abstract = { Adam is a popular variant of stochastic gradient descent for finding a local minimizer of a function. In the constant stepsize regime, assuming that the objective function is differentiable and nonconvex, we establish the convergence in the long run of the iterates to a stationary point under a stability condition. The key ingredient is the introduction of a continuous-time version of Adam, under the form of a nonautonomous ordinary differential equation. This continuous-time system is a relevant approximation of the Adam iterates, in the sense that the interpolated Adam process converges weakly toward the solution to the ODE. The existence and the uniqueness of the solution are established. We further show the convergence of the solution toward the critical points of the objective function and quantify its convergence rate under a ≈Åojasiewicz assumption. Then, we introduce a novel decreasing stepsize version of Adam. Under mild assumptions, it is shown that the iterates are almost surely bounded and converge almost surely to critical points of the objective function. Finally, we analyze the fluctuations of the algorithm by means of a conditional central limit theorem. }
}

@misc{ward2018adagrad,
    title={AdaGrad stepsizes: Sharp convergence over nonconvex landscapes},
    author={Rachel Ward and Xiaoxia Wu and Leon Bottou},
    year={2018},
    eprint={1806.01811},
    archivePrefix={arXiv},
    primaryClass={stat.ML}
}